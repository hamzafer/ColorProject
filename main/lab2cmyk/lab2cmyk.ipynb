{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data\n",
    "data_new = pd.read_csv('../../cleaned/APTEC_PC10_CardBoard_2023_v1.csv')\n",
    "\n",
    "# Extract L*a*b* and CMYK values\n",
    "X_new = data_new[['LAB_L', 'LAB_A', 'LAB_B']]\n",
    "y_new = data_new[['CMYK_C', 'CMYK_M', 'CMYK_Y', 'CMYK_K']]\n",
    "\n",
    "# Standardize the input features\n",
    "scaler_new = StandardScaler()\n",
    "X_scaled_new = scaler_new.fit_transform(X_new)\n",
    "\n",
    "# Split the data\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_scaled_new, y_new, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-27 03:05:17.013592: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 03:05:17.734080: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-27 03:05:17.734189: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-27 03:05:17.884019: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-27 03:05:17.991129: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-27 03:05:18.005713: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-27 03:05:19.422511: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "130/130 [==============================] - 1s 1ms/step - loss: 1304.9158\n",
      "Epoch 2/100\n",
      "130/130 [==============================] - 0s 901us/step - loss: 367.0802\n",
      "Epoch 3/100\n",
      "130/130 [==============================] - 0s 888us/step - loss: 274.5020\n",
      "Epoch 4/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 240.1693\n",
      "Epoch 5/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 223.9676\n",
      "Epoch 6/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 214.1721\n",
      "Epoch 7/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 208.4072\n",
      "Epoch 8/100\n",
      "130/130 [==============================] - 0s 932us/step - loss: 203.5398\n",
      "Epoch 9/100\n",
      "130/130 [==============================] - 0s 983us/step - loss: 201.5960\n",
      "Epoch 10/100\n",
      "130/130 [==============================] - 0s 938us/step - loss: 198.3813\n",
      "Epoch 11/100\n",
      "130/130 [==============================] - 0s 985us/step - loss: 196.9208\n",
      "Epoch 12/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 198.3363\n",
      "Epoch 13/100\n",
      "130/130 [==============================] - 0s 936us/step - loss: 195.5418\n",
      "Epoch 14/100\n",
      "130/130 [==============================] - 0s 988us/step - loss: 192.9701\n",
      "Epoch 15/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 194.8145\n",
      "Epoch 16/100\n",
      "130/130 [==============================] - 0s 985us/step - loss: 191.4433\n",
      "Epoch 17/100\n",
      "130/130 [==============================] - 0s 977us/step - loss: 190.2460\n",
      "Epoch 18/100\n",
      "130/130 [==============================] - 0s 810us/step - loss: 190.2473\n",
      "Epoch 19/100\n",
      "130/130 [==============================] - 0s 917us/step - loss: 188.3402\n",
      "Epoch 20/100\n",
      "130/130 [==============================] - 0s 980us/step - loss: 187.2598\n",
      "Epoch 21/100\n",
      "130/130 [==============================] - 0s 961us/step - loss: 187.4485\n",
      "Epoch 22/100\n",
      "130/130 [==============================] - 0s 934us/step - loss: 188.6630\n",
      "Epoch 23/100\n",
      "130/130 [==============================] - 0s 956us/step - loss: 186.3602\n",
      "Epoch 24/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 187.2318\n",
      "Epoch 25/100\n",
      "130/130 [==============================] - 0s 862us/step - loss: 185.0099\n",
      "Epoch 26/100\n",
      "130/130 [==============================] - 0s 979us/step - loss: 183.9070\n",
      "Epoch 27/100\n",
      "130/130 [==============================] - 0s 971us/step - loss: 186.0200\n",
      "Epoch 28/100\n",
      "130/130 [==============================] - 0s 932us/step - loss: 182.2903\n",
      "Epoch 29/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 184.4755\n",
      "Epoch 30/100\n",
      "130/130 [==============================] - 0s 955us/step - loss: 182.6953\n",
      "Epoch 31/100\n",
      "130/130 [==============================] - 0s 871us/step - loss: 180.3211\n",
      "Epoch 32/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 182.3430\n",
      "Epoch 33/100\n",
      "130/130 [==============================] - 0s 897us/step - loss: 178.8447\n",
      "Epoch 34/100\n",
      "130/130 [==============================] - 0s 904us/step - loss: 180.1599\n",
      "Epoch 35/100\n",
      "130/130 [==============================] - 0s 957us/step - loss: 179.1686\n",
      "Epoch 36/100\n",
      "130/130 [==============================] - 0s 874us/step - loss: 180.1272\n",
      "Epoch 37/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 179.8051\n",
      "Epoch 38/100\n",
      "130/130 [==============================] - 0s 923us/step - loss: 178.6237\n",
      "Epoch 39/100\n",
      "130/130 [==============================] - 0s 997us/step - loss: 176.9101\n",
      "Epoch 40/100\n",
      "130/130 [==============================] - 0s 858us/step - loss: 179.6563\n",
      "Epoch 41/100\n",
      "130/130 [==============================] - 0s 918us/step - loss: 177.1461\n",
      "Epoch 42/100\n",
      "130/130 [==============================] - 0s 925us/step - loss: 175.1243\n",
      "Epoch 43/100\n",
      "130/130 [==============================] - 0s 940us/step - loss: 176.1887\n",
      "Epoch 44/100\n",
      "130/130 [==============================] - 0s 930us/step - loss: 176.6507\n",
      "Epoch 45/100\n",
      "130/130 [==============================] - 0s 883us/step - loss: 173.7181\n",
      "Epoch 46/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 175.6934\n",
      "Epoch 47/100\n",
      "130/130 [==============================] - 0s 951us/step - loss: 174.6451\n",
      "Epoch 48/100\n",
      "130/130 [==============================] - 0s 966us/step - loss: 174.1088\n",
      "Epoch 49/100\n",
      "130/130 [==============================] - 0s 850us/step - loss: 171.8650\n",
      "Epoch 50/100\n",
      "130/130 [==============================] - 0s 901us/step - loss: 172.2018\n",
      "Epoch 51/100\n",
      "130/130 [==============================] - 0s 934us/step - loss: 172.3470\n",
      "Epoch 52/100\n",
      "130/130 [==============================] - 0s 826us/step - loss: 172.1550\n",
      "Epoch 53/100\n",
      "130/130 [==============================] - 0s 893us/step - loss: 170.2978\n",
      "Epoch 54/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 169.7391\n",
      "Epoch 55/100\n",
      "130/130 [==============================] - 0s 916us/step - loss: 170.7798\n",
      "Epoch 56/100\n",
      "130/130 [==============================] - 0s 935us/step - loss: 169.6485\n",
      "Epoch 57/100\n",
      "130/130 [==============================] - 0s 955us/step - loss: 170.9860\n",
      "Epoch 58/100\n",
      "130/130 [==============================] - 0s 998us/step - loss: 168.6727\n",
      "Epoch 59/100\n",
      "130/130 [==============================] - 0s 934us/step - loss: 167.7396\n",
      "Epoch 60/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 168.4949\n",
      "Epoch 61/100\n",
      "130/130 [==============================] - 0s 909us/step - loss: 168.1367\n",
      "Epoch 62/100\n",
      "130/130 [==============================] - 0s 984us/step - loss: 168.1861\n",
      "Epoch 63/100\n",
      "130/130 [==============================] - 0s 960us/step - loss: 167.3631\n",
      "Epoch 64/100\n",
      "130/130 [==============================] - 0s 972us/step - loss: 170.2509\n",
      "Epoch 65/100\n",
      "130/130 [==============================] - 0s 962us/step - loss: 166.1387\n",
      "Epoch 66/100\n",
      "130/130 [==============================] - 0s 968us/step - loss: 166.6647\n",
      "Epoch 67/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 164.7268\n",
      "Epoch 68/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 166.3942\n",
      "Epoch 69/100\n",
      "130/130 [==============================] - 0s 810us/step - loss: 164.5786\n",
      "Epoch 70/100\n",
      "130/130 [==============================] - 0s 827us/step - loss: 163.1375\n",
      "Epoch 71/100\n",
      "130/130 [==============================] - 0s 945us/step - loss: 164.5642\n",
      "Epoch 72/100\n",
      "130/130 [==============================] - 0s 909us/step - loss: 164.0491\n",
      "Epoch 73/100\n",
      "130/130 [==============================] - 0s 905us/step - loss: 163.3081\n",
      "Epoch 74/100\n",
      "130/130 [==============================] - 0s 917us/step - loss: 163.5787\n",
      "Epoch 75/100\n",
      "130/130 [==============================] - 0s 926us/step - loss: 163.0905\n",
      "Epoch 76/100\n",
      "130/130 [==============================] - 0s 926us/step - loss: 164.2277\n",
      "Epoch 77/100\n",
      "130/130 [==============================] - 0s 862us/step - loss: 162.4926\n",
      "Epoch 78/100\n",
      "130/130 [==============================] - 0s 846us/step - loss: 162.7591\n",
      "Epoch 79/100\n",
      "130/130 [==============================] - 0s 933us/step - loss: 160.0813\n",
      "Epoch 80/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 160.8283\n",
      "Epoch 81/100\n",
      "130/130 [==============================] - 0s 865us/step - loss: 161.5971\n",
      "Epoch 82/100\n",
      "130/130 [==============================] - 0s 941us/step - loss: 161.9895\n",
      "Epoch 83/100\n",
      "130/130 [==============================] - 0s 968us/step - loss: 160.2634\n",
      "Epoch 84/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 159.6209\n",
      "Epoch 85/100\n",
      "130/130 [==============================] - 0s 915us/step - loss: 161.7182\n",
      "Epoch 86/100\n",
      "130/130 [==============================] - 0s 903us/step - loss: 158.3763\n",
      "Epoch 87/100\n",
      "130/130 [==============================] - 0s 921us/step - loss: 159.9970\n",
      "Epoch 88/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 159.5466\n",
      "Epoch 89/100\n",
      "130/130 [==============================] - 0s 920us/step - loss: 157.7506\n",
      "Epoch 90/100\n",
      "130/130 [==============================] - 0s 921us/step - loss: 159.7073\n",
      "Epoch 91/100\n",
      "130/130 [==============================] - 0s 961us/step - loss: 158.3713\n",
      "Epoch 92/100\n",
      "130/130 [==============================] - 0s 864us/step - loss: 156.2003\n",
      "Epoch 93/100\n",
      "130/130 [==============================] - 0s 887us/step - loss: 156.3767\n",
      "Epoch 94/100\n",
      "130/130 [==============================] - 0s 865us/step - loss: 158.2828\n",
      "Epoch 95/100\n",
      "130/130 [==============================] - 0s 868us/step - loss: 157.2982\n",
      "Epoch 96/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 157.8727\n",
      "Epoch 97/100\n",
      "130/130 [==============================] - 0s 1ms/step - loss: 155.1853\n",
      "Epoch 98/100\n",
      "130/130 [==============================] - 0s 879us/step - loss: 156.2234\n",
      "Epoch 99/100\n",
      "130/130 [==============================] - 0s 880us/step - loss: 156.9756\n",
      "Epoch 100/100\n",
      "130/130 [==============================] - 0s 994us/step - loss: 155.3753\n",
      "11/11 [==============================] - 0s 842us/step - loss: 175.1151\n",
      "Test loss: 175.1150665283203\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Neural network architecture\n",
    "model_new = Sequential([\n",
    "    Dense(128, input_dim=3, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(4)  # Output layer for CMYK\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_new.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model_new.fit(X_train_new, y_train_new, epochs=100, batch_size=10)\n",
    "\n",
    "# Evaluate the model\n",
    "loss_new = model_new.evaluate(X_test_new, y_test_new)\n",
    "print(f'Test loss: {loss_new}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 1872.3943 - val_loss: 891.4718\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 685.3067 - val_loss: 446.7178\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 405.3547 - val_loss: 322.4778\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 347.5673 - val_loss: 281.1400\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 317.9138 - val_loss: 260.9294\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 302.8807 - val_loss: 244.7726\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 290.2832 - val_loss: 232.1157\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 273.6054 - val_loss: 223.8694\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 270.8780 - val_loss: 214.0304\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 262.5820 - val_loss: 206.4350\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 262.4228 - val_loss: 201.0995\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 255.3749 - val_loss: 199.7675\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 254.5817 - val_loss: 205.7120\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 253.5231 - val_loss: 199.4992\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 245.2507 - val_loss: 187.2751\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 248.1721 - val_loss: 191.5490\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 247.6487 - val_loss: 187.1965\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 244.8046 - val_loss: 191.8878\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 238.9675 - val_loss: 187.2962\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 241.1752 - val_loss: 181.2090\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 239.1449 - val_loss: 180.9537\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 237.2764 - val_loss: 183.6485\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 241.5192 - val_loss: 184.3493\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 236.2851 - val_loss: 181.1116\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 239.4904 - val_loss: 186.7533\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 236.3684 - val_loss: 179.4787\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 236.3456 - val_loss: 185.1454\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 232.8548 - val_loss: 176.5653\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 232.3350 - val_loss: 177.0577\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 235.1616 - val_loss: 186.6096\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 231.6501 - val_loss: 173.1243\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 233.4116 - val_loss: 174.5309\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 221.5851 - val_loss: 178.5057\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 225.8427 - val_loss: 181.2671\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 232.4980 - val_loss: 176.3823\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 225.6118 - val_loss: 174.6372\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 222.9931 - val_loss: 175.6572\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 227.2965 - val_loss: 181.6582\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 224.8354 - val_loss: 182.1087\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 223.7156 - val_loss: 177.1182\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 226.4058 - val_loss: 170.4304\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 226.3561 - val_loss: 174.3990\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 228.3334 - val_loss: 175.8146\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 219.9303 - val_loss: 170.9540\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 218.4970 - val_loss: 173.3504\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 226.1747 - val_loss: 171.2320\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 219.4150 - val_loss: 175.0487\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 217.3624 - val_loss: 172.7233\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 220.0817 - val_loss: 170.2519\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 218.3159 - val_loss: 170.4992\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 221.0888 - val_loss: 169.9805\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 218.6363 - val_loss: 170.9940\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 218.9097 - val_loss: 170.2147\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 220.7160 - val_loss: 170.3963\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 213.6864 - val_loss: 173.0735\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 223.4956 - val_loss: 170.2703\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 214.1282 - val_loss: 170.5910\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 213.0833 - val_loss: 169.8975\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 219.0838 - val_loss: 180.0536\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 215.0164 - val_loss: 168.9716\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 217.3077 - val_loss: 170.6555\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 213.7473 - val_loss: 166.4961\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 210.2471 - val_loss: 169.0392\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 217.1526 - val_loss: 175.2445\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 215.8052 - val_loss: 168.9432\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 216.3655 - val_loss: 173.8491\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 220.3998 - val_loss: 167.2283\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 220.3997 - val_loss: 166.7845\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 213.1735 - val_loss: 169.2374\n",
      "Epoch 70/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 209.9345 - val_loss: 171.9155\n",
      "Epoch 71/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 211.2534 - val_loss: 166.2681\n",
      "Epoch 72/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 214.6398 - val_loss: 175.6860\n",
      "Epoch 73/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 216.4862 - val_loss: 169.2421\n",
      "Epoch 74/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 211.0579 - val_loss: 167.1484\n",
      "Epoch 75/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 215.8227 - val_loss: 165.2695\n",
      "Epoch 76/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 217.3853 - val_loss: 166.3368\n",
      "Epoch 77/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 213.7329 - val_loss: 165.2903\n",
      "Epoch 78/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 212.5105 - val_loss: 165.8815\n",
      "Epoch 79/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 213.4044 - val_loss: 171.3580\n",
      "Epoch 80/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 207.4650 - val_loss: 167.0861\n",
      "Epoch 81/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 214.3582 - val_loss: 165.9837\n",
      "Epoch 82/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 208.3698 - val_loss: 168.6949\n",
      "Epoch 83/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 207.8267 - val_loss: 164.3562\n",
      "Epoch 84/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 210.0387 - val_loss: 169.3034\n",
      "Epoch 85/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 207.0598 - val_loss: 164.4608\n",
      "Epoch 86/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 208.1420 - val_loss: 165.7284\n",
      "Epoch 87/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 209.5275 - val_loss: 172.1704\n",
      "Epoch 88/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 212.0359 - val_loss: 167.6683\n",
      "Epoch 89/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 206.5828 - val_loss: 172.1485\n",
      "Epoch 90/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 208.1543 - val_loss: 168.6665\n",
      "Epoch 91/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 209.0857 - val_loss: 164.4356\n",
      "Epoch 92/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 208.7594 - val_loss: 165.5433\n",
      "Epoch 93/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 209.9677 - val_loss: 168.1050\n",
      "11/11 [==============================] - 0s 1ms/step - loss: 186.7059\n",
      "Test loss: 186.7058563232422\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load the data\n",
    "data_new = pd.read_csv('../../cleaned/APTEC_PC10_CardBoard_2023_v1.csv')\n",
    "\n",
    "# Extract L*a*b* and CMYK values\n",
    "X_new = data_new[['LAB_L', 'LAB_A', 'LAB_B']]\n",
    "y_new = data_new[['CMYK_C', 'CMYK_M', 'CMYK_Y', 'CMYK_K']]\n",
    "\n",
    "# Standardize the input features\n",
    "scaler_new = StandardScaler()\n",
    "X_scaled_new = scaler_new.fit_transform(X_new)\n",
    "\n",
    "# Split the data\n",
    "X_train_new, X_test_new, y_train_new, y_test_new = train_test_split(X_scaled_new, y_new, test_size=0.2, random_state=42)\n",
    "\n",
    "# Enhanced neural network architecture\n",
    "model_new = Sequential([\n",
    "    Dense(256, input_dim=3, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(4)  # Output layer for CMYK\n",
    "])\n",
    "\n",
    "# Compile the model with an Adam optimizer and a learning rate of 0.001\n",
    "model_new.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model_new.fit(X_train_new, y_train_new, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model\n",
    "loss_new = model_new.evaluate(X_test_new, y_test_new)\n",
    "print(f'Test loss: {loss_new}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "37/37 [==============================] - 1s 6ms/step - loss: 2185.7629 - val_loss: 2317.7551 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 1848.4779 - val_loss: 2164.4365 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 1419.8008 - val_loss: 1846.9343 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 974.9975 - val_loss: 1404.0428 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 679.6280 - val_loss: 1026.1659 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 500.8292 - val_loss: 739.0181 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 386.0446 - val_loss: 555.8965 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 330.3335 - val_loss: 462.3751 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 282.3911 - val_loss: 379.7252 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 269.9539 - val_loss: 332.0343 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 255.2448 - val_loss: 279.3802 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 242.1495 - val_loss: 256.3664 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 234.8426 - val_loss: 247.4557 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 234.0837 - val_loss: 234.0810 - lr: 0.0010\n",
      "Epoch 15/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 238.2199 - val_loss: 197.7782 - lr: 0.0010\n",
      "Epoch 16/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 227.6028 - val_loss: 214.9042 - lr: 0.0010\n",
      "Epoch 17/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 231.2090 - val_loss: 185.1278 - lr: 0.0010\n",
      "Epoch 18/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 219.7276 - val_loss: 190.6901 - lr: 0.0010\n",
      "Epoch 19/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 229.0071 - val_loss: 179.5647 - lr: 0.0010\n",
      "Epoch 20/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 214.5863 - val_loss: 183.2360 - lr: 0.0010\n",
      "Epoch 21/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 218.9445 - val_loss: 177.3243 - lr: 0.0010\n",
      "Epoch 22/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 218.1114 - val_loss: 177.4715 - lr: 0.0010\n",
      "Epoch 23/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 213.9856 - val_loss: 176.6110 - lr: 0.0010\n",
      "Epoch 24/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 219.1487 - val_loss: 189.8938 - lr: 0.0010\n",
      "Epoch 25/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 213.4641 - val_loss: 174.2787 - lr: 0.0010\n",
      "Epoch 26/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 212.5581 - val_loss: 174.7675 - lr: 0.0010\n",
      "Epoch 27/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 213.0523 - val_loss: 172.0221 - lr: 0.0010\n",
      "Epoch 28/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 214.3365 - val_loss: 171.6427 - lr: 0.0010\n",
      "Epoch 29/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 210.8676 - val_loss: 171.4828 - lr: 0.0010\n",
      "Epoch 30/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 204.0621 - val_loss: 179.3004 - lr: 0.0010\n",
      "Epoch 31/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 214.2754 - val_loss: 172.5926 - lr: 0.0010\n",
      "Epoch 32/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 213.7326 - val_loss: 179.9924 - lr: 0.0010\n",
      "Epoch 33/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 209.1935 - val_loss: 176.0444 - lr: 0.0010\n",
      "Epoch 34/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 204.8878 - val_loss: 175.3128 - lr: 0.0010\n",
      "Epoch 35/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 200.3474 - val_loss: 172.7191 - lr: 5.0000e-04\n",
      "Epoch 36/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 208.4124 - val_loss: 171.4965 - lr: 5.0000e-04\n",
      "Epoch 37/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 198.8734 - val_loss: 172.1457 - lr: 5.0000e-04\n",
      "Epoch 38/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 202.3526 - val_loss: 173.6792 - lr: 5.0000e-04\n",
      "Epoch 39/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 195.3379 - val_loss: 171.0911 - lr: 5.0000e-04\n",
      "Epoch 40/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 197.9067 - val_loss: 174.0714 - lr: 5.0000e-04\n",
      "Epoch 41/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 196.7504 - val_loss: 169.4836 - lr: 5.0000e-04\n",
      "Epoch 42/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 200.0768 - val_loss: 169.4304 - lr: 5.0000e-04\n",
      "Epoch 43/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 202.4119 - val_loss: 171.7004 - lr: 5.0000e-04\n",
      "Epoch 44/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 195.7222 - val_loss: 168.1254 - lr: 5.0000e-04\n",
      "Epoch 45/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 199.3491 - val_loss: 169.1554 - lr: 5.0000e-04\n",
      "Epoch 46/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 200.9473 - val_loss: 168.2618 - lr: 5.0000e-04\n",
      "Epoch 47/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 196.1906 - val_loss: 169.2819 - lr: 5.0000e-04\n",
      "Epoch 48/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 197.7234 - val_loss: 171.2381 - lr: 5.0000e-04\n",
      "Epoch 49/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 189.5409 - val_loss: 167.5395 - lr: 5.0000e-04\n",
      "Epoch 50/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 201.1227 - val_loss: 168.8520 - lr: 5.0000e-04\n",
      "Epoch 51/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 202.2643 - val_loss: 171.1521 - lr: 5.0000e-04\n",
      "Epoch 52/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 196.4636 - val_loss: 172.3287 - lr: 5.0000e-04\n",
      "Epoch 53/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 195.8714 - val_loss: 167.9412 - lr: 5.0000e-04\n",
      "Epoch 54/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 189.0585 - val_loss: 168.9593 - lr: 5.0000e-04\n",
      "Epoch 55/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 189.2331 - val_loss: 166.2207 - lr: 2.5000e-04\n",
      "Epoch 56/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 189.2407 - val_loss: 167.4406 - lr: 2.5000e-04\n",
      "Epoch 57/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 180.2382 - val_loss: 167.0048 - lr: 2.5000e-04\n",
      "Epoch 58/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 189.3766 - val_loss: 166.4226 - lr: 2.5000e-04\n",
      "Epoch 59/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 187.1008 - val_loss: 165.7656 - lr: 2.5000e-04\n",
      "Epoch 60/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 184.9969 - val_loss: 169.6532 - lr: 2.5000e-04\n",
      "Epoch 61/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 184.4501 - val_loss: 169.8365 - lr: 2.5000e-04\n",
      "Epoch 62/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 189.1676 - val_loss: 168.0224 - lr: 2.5000e-04\n",
      "Epoch 63/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 190.2764 - val_loss: 167.2811 - lr: 2.5000e-04\n",
      "Epoch 64/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 191.1226 - val_loss: 166.8276 - lr: 2.5000e-04\n",
      "Epoch 65/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 183.2224 - val_loss: 168.3911 - lr: 1.2500e-04\n",
      "Epoch 66/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 187.4585 - val_loss: 168.4488 - lr: 1.2500e-04\n",
      "Epoch 67/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 188.8276 - val_loss: 166.7140 - lr: 1.2500e-04\n",
      "Epoch 68/100\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 182.1618 - val_loss: 166.7140 - lr: 1.2500e-04\n",
      "Epoch 69/100\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 185.3792 - val_loss: 168.1446 - lr: 1.2500e-04\n",
      "11/11 [==============================] - 0s 964us/step - loss: 175.5519\n",
      "Adjusted Test loss: 175.5518798828125\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Adjusted neural network architecture\n",
    "model_adjusted = Sequential([\n",
    "    Dense(128, input_dim=3, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(4)  # Output layer for CMYK\n",
    "])\n",
    "\n",
    "# Compile the model with an Adam optimizer and a learning rate of 0.001\n",
    "model_adjusted.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Learning rate scheduler\n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "\n",
    "# Train the model with adjusted architecture\n",
    "model_adjusted.fit(X_train_new, y_train_new, epochs=100, batch_size=32, validation_split=0.1, callbacks=[early_stopping, lr_scheduler])\n",
    "\n",
    "# Evaluate the model\n",
    "loss_adjusted = model_adjusted.evaluate(X_test_new, y_test_new)\n",
    "print(f'Adjusted Test loss: {loss_adjusted}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (6,), 'activation': 'logistic', 'solver': 'lbfgs', 'max_iter': 1000}, MSE: 202.871010956031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (6,), 'activation': 'logistic', 'solver': 'adam', 'max_iter': 1000}, MSE: 879.7723905498374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (6,), 'activation': 'relu', 'solver': 'adam', 'max_iter': 1000}, MSE: 250.9392805690731\n",
      "Configuration: {'hidden_layer_sizes': (6,), 'activation': 'tanh', 'solver': 'sgd', 'max_iter': 1000}, MSE: 294.60757436839253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (10,), 'activation': 'relu', 'solver': 'adam', 'max_iter': 200}, MSE: 735.2935814231735\n",
      "Configuration: {'hidden_layer_sizes': (50, 30, 10), 'activation': 'tanh', 'solver': 'sgd', 'max_iter': 500}, MSE: 237.38409918028523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (10,), 'activation': 'relu', 'solver': 'adam', 'max_iter': 200}, MSE: 735.2935814231735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (20, 10), 'activation': 'relu', 'solver': 'adam', 'max_iter': 300}, MSE: 230.47109065446435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (400) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (30, 20, 10), 'activation': 'relu', 'solver': 'adam', 'max_iter': 400}, MSE: 197.04047427043082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (20,), 'activation': 'tanh', 'solver': 'adam', 'max_iter': 200}, MSE: 1101.578639261582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (20,), 'activation': 'logistic', 'solver': 'adam', 'max_iter': 200}, MSE: 1195.3278924238311\n",
      "Configuration: {'hidden_layer_sizes': (20,), 'activation': 'relu', 'solver': 'sgd', 'max_iter': 500}, MSE: 182.1344878309472\n",
      "Configuration: {'hidden_layer_sizes': (20,), 'activation': 'relu', 'solver': 'lbfgs', 'max_iter': 200}, MSE: 176.74061497924188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:546: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'hidden_layer_sizes': (20,), 'activation': 'relu', 'solver': 'adam', 'max_iter': 1000}, MSE: 224.49215418674697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/minhas/work/colorProject/ColorProject/.venv/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
